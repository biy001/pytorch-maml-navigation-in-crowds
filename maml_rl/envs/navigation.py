import numpy as np

import gym
from gym import spaces
from gym.utils import seeding

class Navigation2DEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from 
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
    position (ie. the reward is `-distance`). The 2D navigation tasks are 
    generated by sampling goal positions from the uniform distribution 
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic 
        Meta-Learning for Fast Adaptation of Deep Networks", 2017 
        (https://arxiv.org/abs/1703.03400)
    """
    """
    What's new for the new environment:
    Added 8 pedestrians initialized to be at 4 corners ([-0.8,-0.8], [0.8,-0.8], [0.8,0.8], [-0.8,0.8]) 
    of a rectangle centering at the origin. 2 pedestrians at each corner. They walk almostly 
    diagonally towards the other side (specific direction is upon randomness). After they exit the rectangle, 
    they will be initialized at the corners again. 
    """
    
    def __init__(self, task={}):
        super(Navigation2DEnv, self).__init__()
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
            shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-0.1, high=0.1,
            shape=(2,), dtype=np.float32)

        self._task = task
        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))
        self._state = np.zeros(2, dtype=np.float32)
        self.seed()



        self._ped_speed = task.get('ped_speed', np.float32(0))
        self._ped_direc = task.get('ped_direc', np.zeros(8, dtype=np.float32))
        # ----------- NEW ENV --------------------
        # self._n_pedestrian = 8 # or use np.random.randint but needs to adjust _ped_state
        self._entering_corner = np.float32(0.8)
        self._default_ped_state = self._entering_corner * np.repeat(np.array([[-1,-1], [1,-1], [1,1], [-1,1]]), 2, axis=0)
        self._ped_state = self._default_ped_state.copy()
        # ----------- Finish NEW ENV --------------------
        
    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def sample_tasks(self, num_tasks):
        goals = self.np_random.uniform(-0.5, 0.5, size=(num_tasks, 2))
        
        # ----------- NEW ENV --------------------
        ped_speeds = self.np_random.uniform(0.0, 0.4, size=num_tasks)
        ped_direcs = np.zeros((num_tasks, 8), dtype=np.float32) # 8 pedestrians

        ped_direcs = self.np_random.uniform(0.0, np.pi/2, size=(num_tasks, 2))
        for i in range(3):
            temp_rand = self.np_random.uniform((i+1)*np.pi/2, (i+2)*np.pi/2, size=(num_tasks, 2))
        # nPedestrian = np.random.randint(3, 7, size=num_tasks)
        tasks = [{'goal': goal, 'ped_speed': ped_speed, 'ped_direc': ped_direc} for goal, ped_speed, ped_direc in zip(goals, ped_speeds, ped_direcs)]
        # ----------- Finish NEW ENV --------------------
        return tasks

    def reset_task(self, task):
        self._task = task
        self._goal = task['goal']
        self._ped_speed = task['ped_speed']
        self._ped_direc = task['ped_direc']

    def reset(self, env=True):
        self._state = np.zeros(2, dtype=np.float32)
        return self._state

    def step(self, action):
        action = np.clip(action, -0.1, 0.1)

        try: # for debugging. Not sure why it gives assertion error sometimes in the middle of training...
            assert self.action_space.contains(action)
        except AssertionError as error:
            print("AssertionError: action is {}".format(action))

        assert self.action_space.contains(action)
        self._state = self._state + action
        x = self._state[0] - self._goal[0]
        y = self._state[1] - self._goal[1]
        # ----------- NEW ENV --------------------
        reward = np.float32(0)
        for i in range(self._ped_direc.shape[0]):
            if any(abs(self._ped_state[i,:]) > self._entering_corner + 0.0001):
                self._ped_state[[i, i], [0, 1]] = self._default_ped_state[i,:]
                # self._ped_direc[i] = self.np_random.uniform(int(i/2)*np.pi/2, (int(i/2)+1)*np.pi/2, size=1)
            else:
                self._ped_state[[i, i], [0, 1]] = self._ped_state[i,:] + self._ped_speed* np.array([np.cos(self._ped_direc[i]), np.sin(self._ped_direc[i])])
            dist_ped_i = np.sqrt((self._ped_state[i,0] - self._state[0]) ** 2 + (self._ped_state[i,1] - self._state[1]) ** 2)
            if (dist_ped_i < 0.1): # assume safe distance is within a radius of 0.1
                reward = reward - 0.1 + dist_ped_i*1.0

        reward = reward - np.sqrt(x ** 2 + y ** 2)
        # ----------- Finish NEW ENV --------------------
        done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))

        return self._state, reward, done, self._task
